{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Object_detection_pets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemantha-kumara/tensorflow-objectdetection/blob/master/Object_detection_pets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmyU7Wr6kS47",
        "colab_type": "text"
      },
      "source": [
        "# Dependencies installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bng8JmgCBdsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install contextlib2 lxml pillow matplotlib Cython "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGlhYlx4g_16",
        "colab_type": "text"
      },
      "source": [
        "# Getting the Oxford-IIIT Pets Dataset and Uploading it to local /content path\n",
        "\n",
        "In order to train a detector, we require a dataset of images, bounding boxes and classifications. For this demo, we'll use the Oxford-IIIT Pets dataset.  We will need to download both the image dataset images.tar.gz and the groundtruth data annotations.tar.gz to the tensorflow/models/research/ directory and unzip them. This may take some time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bASx1wtD9QVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz  -P /content --no-check-certificate\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz  -P /content --no-check-certificate\n",
        "%cd /content\n",
        "\n",
        "!tar -C /content -xvf /content/images.tar.gz\n",
        "!tar -C /content -xvf /content/annotations.tar.gz\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7sdiHu3huvM",
        "colab_type": "text"
      },
      "source": [
        "# Downloading a COCO-pretrained Model for Transfer Learning\n",
        "Training a state of the art object detector from scratch can take days, even when using multiple GPUs! In order to speed up training, we'll take an object detector trained on a different dataset (COCO), and reuse some of it's parameters to initialize our new model.\n",
        "\n",
        "Download our COCO-pretrained Faster R-CNN with Resnet-101 model. Unzip the contents of the folder and copy the model.ckpt* files into your /content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c1Up0RIiHkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_2018_01_28.tar.gz  -P /content --no-check-certificate\n",
        "!tar -C /content -xvf /content/faster_rcnn_resnet101_coco_2018_01_28.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVVnt3FijBAm",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO60iDMK-mzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/code\n",
        "!git clone https://github.com/tensorflow/models.git /content/code\n",
        "%cd /content/code\n",
        "!git checkout 81123eb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zDQARjZi43H",
        "colab_type": "text"
      },
      "source": [
        "# Protobuf Compilation\n",
        "The Tensorflow Object Detection API uses Protobufs to configure model and training parameters. Before the framework can be used, the Protobuf libraries must be compiled. This should be done by running the following command from the tensorflow/models/research/ directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAUo6GES-zoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/code/research\n",
        "!wget https://github.com/google/protobuf/releases/download/v3.0.0/protoc-3.0.0-linux-x86_64.zip\n",
        "!unzip protoc-3.0.0-linux-x86_64.zip\n",
        "!./bin/protoc object_detection/protos/*.proto --python_out=.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJDsIIM1jOFa",
        "colab_type": "text"
      },
      "source": [
        "# Add Libraries to PYTHONPATH\n",
        "When running locally, the tensorflow/models/research/ and slim directories should be appended to PYTHONPATH. This can be done by running the following from models/research/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr1vXFy4Cq4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export PYTHONPATH=$PYTHONPATH:/content/code/research:/content/code/research/slim\n",
        "%cd /content/code/research/\n",
        "!python setup.py build\n",
        "!python setup.py install\n",
        "%cd /content/code/research/slim\n",
        "!pip install -e ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIgRoAAkjoPn",
        "colab_type": "text"
      },
      "source": [
        "# Creation of TFRecords\n",
        "The Tensorflow Object Detection API expects data to be in the TFRecord format, so we'll now run the create_pet_tf_record script to convert from the raw Oxford-IIIT Pet dataset into TFRecords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It5zChR4BoiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python /content/code/research/object_detection/dataset_tools/create_pet_tf_record.py --label_map_path=/content/code/research/object_detection/data/pet_label_map.pbtxt --data_dir=/content --output_dir=/content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KU7GYPlj0ie",
        "colab_type": "text"
      },
      "source": [
        "# Starting Tensorflow Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNJrWoZkEsiK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "534d019e-7191-46de-9d59-fdae28286e93"
      },
      "source": [
        "!python /content/code/research/object_detection/legacy/train.py --pipeline_config_path=/content/faster_rcnn_resnet101_pets.config --train_dir=/content/train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global step 211: loss = 2.2750 (19.881 sec/step)\n",
            "I0916 15:09:33.304884 139651553335168 learning.py:507] global step 211: loss = 2.2750 (19.881 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 211.\n",
            "I0916 15:09:38.953915 139650327602944 supervisor.py:1050] Recording summary at step 211.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxPjh72Lksvn",
        "colab_type": "text"
      },
      "source": [
        "# Exporting the Tensorflow Graph\n",
        "After your model has been trained, you should export it to a Tensorflow graph proto. First, you need to identify a candidate checkpoint to export.\n",
        "checkpoint files can be found in /content/train/\n",
        "The checkpoint will typically consist of three files:\n",
        "```\n",
        "model.ckpt-${CHECKPOINT_NUMBER}.data-00000-of-00001\n",
        "model.ckpt-${CHECKPOINT_NUMBER}.index\n",
        "model.ckpt-${CHECKPOINT_NUMBER}.meta\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5MhxAUDNAee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%env CHECKPOINT_NUMBER=99\n",
        "!python /content/code/research/object_detection/export_inference_graph.py --input_type=image_tensor --pipeline_config_path=/content/faster_rcnn_resnet101_pets.config --trained_checkpoint_prefix=/content/train/model.ckpt-${CHECKPOINT_NUMBER} --output_directory=/content/exported_graphs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3mGWelIOw8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!tensorboard --logdir=file:///content/exported_graphs/\n",
        "LOG_DIR = 'file:///content/exported_graphs/'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "# Install\n",
        "! npm install -g localtunnel\n",
        "\n",
        "# Tunnel port 6006 (TensorBoard assumed running)\n",
        "get_ipython().system_raw('lt --port 6006 >> url.txt 2>&1 &')\n",
        "\n",
        "# Get url\n",
        "! cat url.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIyP_2H9luXP",
        "colab_type": "text"
      },
      "source": [
        "# object detection inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcC4sqfjlwmc",
        "colab_type": "text"
      },
      "source": [
        "**imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1WNDEX6R-_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from distutils.version import StrictVersion\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "if StrictVersion(tf.__version__) < StrictVersion('1.12.0'):\n",
        "  raise ImportError('Please upgrade your TensorFlow installation to v1.12.*.')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHxX3Dp5l7kw",
        "colab_type": "text"
      },
      "source": [
        "# Env setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcdAN8wNSFCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is needed to display the images.\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lzSl1mLl9fM",
        "colab_type": "text"
      },
      "source": [
        "# Object detection imports\n",
        "Here are the imports from the object detection module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVfYubKlSI4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-36vdutPmFHf",
        "colab_type": "text"
      },
      "source": [
        "# Model preparation\n",
        "Any model exported using the export_inference_graph.py tool can be loaded here simply by changing PATH_TO_FROZEN_GRAPH to point to a new .pb file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOYPtSgtSTIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
        "PATH_TO_FROZEN_GRAPH = '/content/exported_graphs/frozen_inference_graph.pb'\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = '/content/code/research/object_detection/data/pet_label_map.pbtxt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJSznEpymRNN",
        "colab_type": "text"
      },
      "source": [
        "# Load a (frozen) Tensorflow model into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyIFk1CBSolX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "  od_graph_def = tf.GraphDef()\n",
        "  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
        "    serialized_graph = fid.read()\n",
        "    od_graph_def.ParseFromString(serialized_graph)\n",
        "    tf.import_graph_def(od_graph_def, name='')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwTNKdgnmTFb",
        "colab_type": "text"
      },
      "source": [
        "# Loading label map\n",
        "Label maps map indices to category names, so that when our convolution network predicts 5, we know that this corresponds to airplane. Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdFyQPWkSqA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGl6ajHzmeT3",
        "colab_type": "text"
      },
      "source": [
        "# Helper code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6mAOwvPT9Wu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_image_into_numpy_array(image):\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE6Uo1blmhKm",
        "colab_type": "text"
      },
      "source": [
        "# Helper github for images ex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79nuQKSvUELb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/final_inference_code\n",
        "!git clone https://github.com/kubeflow/examples.git /content/final_inference_code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh8tO1yMmmXF",
        "colab_type": "text"
      },
      "source": [
        "# Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZGd18lyUjr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For the sake of simplicity we will use only 1 image:\n",
        "TEST_IMAGE_PATHS = ['/content/final_inference_code/object_detection/serving_script/image1.jpg']\n",
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (12, 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGOi1_-9VaVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_inference_for_single_image(image, graph):\n",
        "  with graph.as_default():\n",
        "    with tf.Session() as sess:\n",
        "      # Get handles to input and output tensors\n",
        "      ops = tf.get_default_graph().get_operations()\n",
        "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
        "      tensor_dict = {}\n",
        "      for key in [\n",
        "          'num_detections', 'detection_boxes', 'detection_scores',\n",
        "          'detection_classes', 'detection_masks'\n",
        "      ]:\n",
        "        tensor_name = key + ':0'\n",
        "        if tensor_name in all_tensor_names:\n",
        "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "              tensor_name)\n",
        "      if 'detection_masks' in tensor_dict:\n",
        "        # The following processing is only for single image\n",
        "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
        "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
        "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
        "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
        "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
        "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "            detection_masks, detection_boxes, image.shape[1], image.shape[2])\n",
        "        detection_masks_reframed = tf.cast(\n",
        "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "        # Follow the convention by adding back the batch dimension\n",
        "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "            detection_masks_reframed, 0)\n",
        "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "      # Run inference\n",
        "      output_dict = sess.run(tensor_dict,\n",
        "                             feed_dict={image_tensor: image})\n",
        "\n",
        "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
        "      output_dict['detection_classes'] = output_dict[\n",
        "          'detection_classes'][0].astype(np.int64)\n",
        "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "      if 'detection_masks' in output_dict:\n",
        "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "  return output_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzP8iqrZVbIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for image_path in TEST_IMAGE_PATHS:\n",
        "  image = Image.open(image_path)\n",
        "  # the array based representation of the image will be used later in order to prepare the\n",
        "  # result image with boxes and labels on it.\n",
        "  image_np = load_image_into_numpy_array(image)\n",
        "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "  # Actual detection.\n",
        "  output_dict = run_inference_for_single_image(image_np_expanded, detection_graph)\n",
        "  # Visualization of the results of a detection.\n",
        "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks'),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=8)\n",
        "  plt.figure(figsize=IMAGE_SIZE)\n",
        "  plt.imshow(image_np)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVU5yYvQWCmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/dataDrive/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "247KEHH6bFJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%mkdir /dataDrive/drive/My\\ Drive/colab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwEzkjUvbudh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from distutils.dir_util import copy_tree\n",
        "# copy subdirectory example\n",
        "fromDirectory = \"/content/\"\n",
        "toDirectory = \"/dataDrive/drive/My Drive/colab/\"\n",
        "\n",
        "copy_tree(fromDirectory, toDirectory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJkOK664c5wS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "shutil.rmtree('/content/annotations')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB6A32ridO6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "shutil.rmtree('/content/images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSmleUR-dadm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.remove('/content/annotations.tar.gz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czyLgwKadeae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.remove('/content/images.tar.gz')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}